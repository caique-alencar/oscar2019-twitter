{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qual o termo ou hashtag que você deseja buscar no twitter? #Oscars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scraping hashtag on Twitter\n",
    "\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Carrega as credenciais da API do Twitter\n",
    "# Nesse caso, as credenciais estão em um JSON salvo no mesmo diretório, que vai ser puxado pelo código\n",
    "with open('credenciais_twitter.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['consumer_key']\n",
    "    consumer_secret = info['consumer_secret']\n",
    "    access_token = info['access_token']\n",
    "    access_token_secret = info['access_token_secret']\n",
    "\n",
    "# Autentica as chaves de acesso\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# max_tweets = 10 # Define número máximo de tweets que serão colocados\n",
    "\n",
    "# Pergunta o termo de busca para o usuário\n",
    "termo_de_busca = input(\"Qual o termo ou hashtag que você deseja buscar no twitter? \")\n",
    "print()\n",
    "\n",
    "all_tweets = []\n",
    "oldest = 0\n",
    "\n",
    "# Escreve o CSV\n",
    "with open('tweets_%s.csv' % termo_de_busca, 'w', encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, lineterminator='\\n')\n",
    "    wr.writerow([\"id\", \"datetime\", \"created_at\", \"tweet\"])\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            new_tweets = tweepy.Cursor(api.search, q=termo_de_busca, tweet_mode='extended', lang = \"pt\", rpp = 100, max_id=oldest).items() #items(max_tweets) para definir máximo de tweets\n",
    "            all_tweets = all_tweets.extend(new_tweets)\n",
    "            all_tweets[-1].id - 1\n",
    "            output = [[tweet.id, tweet.created_at, tweet.created_at.strftime(\"%d-%m-%Y %H:%M:%S\"), tweet.full_text] for tweet in all_tweets]\n",
    "            wr.writerow(output)\n",
    "        except tweepy.TweepError:\n",
    "            # Sleep de 15 minutos para não passar do limite de dados da API do Twitter\n",
    "            time.sleep(60 * 15) # Tempo é medido em segundos, por isso multiplica por 60\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(\"Terminei!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qual o termo ou hashtag que você deseja buscar no twitter? #Oscars\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'ItemIterator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6610bf8108b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Pega todos os tweets até não existirem mais tweets restantes para pegar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# len só vai ser 0 quando a request não retornar nada\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Getting tweets before {oldest}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'ItemIterator' has no len()"
     ]
    }
   ],
   "source": [
    "# Scraping hashtag on Twitter\n",
    "\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Carrega as credenciais da API do Twitter\n",
    "# Nesse caso, as credenciais estão em um JSON salvo no mesmo diretório, que vai ser puxado pelo código\n",
    "with open('credenciais_twitter.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['consumer_key']\n",
    "    consumer_secret = info['consumer_secret']\n",
    "    access_token = info['access_token']\n",
    "    access_token_secret = info['access_token_secret']\n",
    "\n",
    "# Autentica as chaves de acesso\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Pergunta o termo de busca para o usuário\n",
    "termo_de_busca = input(\"Qual o termo ou hashtag que você deseja buscar no twitter? \")\n",
    "print()\n",
    "\n",
    "# Abra lista para armazenar os tweets\n",
    "all_tweets = []\n",
    "\n",
    "# Faz a primeira raspagem de tweets\n",
    "new_tweets = tweepy.Cursor(api.search, q=termo_de_busca, tweet_mode='extended', lang = \"pt\", rpp = 100).items()\n",
    "\n",
    "# Pega todos os tweets até não existirem mais tweets restantes para pegar\n",
    "while len(new_tweets) > 0: # len só vai ser 0 quando a request não retornar nada\n",
    "    print(f\"Getting tweets before {oldest}\")\n",
    "        \n",
    "    try:\n",
    "        # Adiciona os novos tweets à lista de todos os tweets\n",
    "        all_tweets.extend(new_tweets)\n",
    "            \n",
    "        # Salva o id do último tweet menos 1\n",
    "        oldest = all_tweets[-1].id - 1\n",
    "            \n",
    "        # Todas as requests subsequentes usam o parâmetro the max_id para evitar duplicatas\n",
    "        new_tweets = tweepy.Cursor(api.search, q=termo_de_busca, tweet_mode='extended', lang=\"pt\", rpp=100, max_id=oldest).items()\n",
    "\n",
    "        # Salva os tweets mais recentes\n",
    "        all_tweets.extend(new_tweets)\n",
    "                \n",
    "        # Atualiza os id do tweet mais antigo menos 1\n",
    "        oldest = all_tweets[-1].id - 1\n",
    "\n",
    "        # Transforma os tweets em um array 2d que vai preencher o CSV\n",
    "        output = [[tweet.id, tweet.created_at, tweet.created_at.strftime(\"%d-%m-%Y %H:%M:%S\"), tweet.full_text] for tweet in all_tweets]\n",
    "        \n",
    "    except tweepy.TweepError:\n",
    "        # Sleep de 15 minutos para não passar do limite de dados da API do Twitter\n",
    "        time.sleep(60 * 15)\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Escreve o CSV\n",
    "with open('tweets_%s.csv' % termo_de_busca, 'w', encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, lineterminator='\\n')\n",
    "    wr.writerow([\"id\", \"datetime\", \"created_at\", \"tweet\"])\n",
    "    wr.writerow(output)\n",
    "\n",
    "print(\"Terminei!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping hashtag on Twitter\n",
    "\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Carrega as credenciais da API do Twitter\n",
    "# Nesse caso, as credenciais estão em um JSON salvo no mesmo diretório, que vai ser puxado pelo código\n",
    "with open('credenciais_twitter.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['consumer_key']\n",
    "    consumer_secret = info['consumer_secret']\n",
    "    access_token = info['access_token']\n",
    "    access_token_secret = info['access_token_secret']\n",
    "\n",
    "# Autentica as chaves de acesso\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# max_tweets = 10 # Define número máximo de tweets que serão colocados\n",
    "\n",
    "# Pergunta o termo de busca para o usuário\n",
    "termo_de_busca = input(\"Qual o termo ou hashtag que você deseja buscar no twitter? \")\n",
    "print()\n",
    "\n",
    "# Escreve o CSV\n",
    "with open('tweets_%s.csv' % termo_de_busca, 'w', encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, lineterminator='\\n')\n",
    "    wr.writerow([\"id\", \"datetime\", \"created_at\", \"tweet\"])\n",
    "    \n",
    "    for tweet in tweepy.Cursor(api.search, q=termo_de_busca, tweet_mode='extended', lang = \"pt\", rpp = 100).items(): #items(max_tweets) para definir máximo de tweets\n",
    "        output = [tweet.id, tweet.created_at, tweet.created_at.strftime(\"%d-%m-%Y %H:%M:%S\"), tweet.full_text]\n",
    "        wr.writerow(output)\n",
    "\n",
    "print(\"Terminei!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter import Twitter, OAuth\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Carrega as credenciais da API do Twitter\n",
    "# Nesse caso, as credenciais estão em um JSON salvo no mesmo diretório, que vai ser puxado pelo código\n",
    "with open('credenciais_twitter.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['consumer_key']\n",
    "    consumer_secret = info['consumer_secret']\n",
    "    access_token = info['access_token']\n",
    "    access_token_secret = info['access_token_secret']\n",
    "\n",
    "t = Twitter(auth = OAuth(access_token, access_token_secret, consumer_key, consumer_secret))\n",
    "\n",
    "resp = t.search.tweets(q=\"#oscars\", lang = \"pt\")\n",
    "\n",
    "for x in resp['statuses']:\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! usr/bin/python3\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from csv import DictWriter\n",
    "import pprint\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def init_driver(driver_type):\n",
    "    if driver_type == 1:\n",
    "        driver = webdriver.Firefox()\n",
    "\n",
    "    elif driver_type == 2:\n",
    "        driver = webdriver.Chrome()\n",
    "\n",
    "    elif driver_type == 3:\n",
    "        driver = webdriver.Ie()\n",
    "\n",
    "    elif driver_type == 4:\n",
    "        driver = webdriver.Opera()\n",
    "\n",
    "    elif driver_type == 5:\n",
    "        driver = webdriver.PhantomJS()\n",
    "\n",
    "    driver.wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    return driver\n",
    "\n",
    "def scroll(driver, start_date, end_date, words, lang, max_time=180):\n",
    "    languages = { 1: 'en', 2: 'it', 3: 'es', 4: 'fr', 5: 'de', 6: 'ru', 7: 'zh', 8: 'pt'}\n",
    "    url = \"https://twitter.com/search?q=\"\n",
    "\n",
    "    for w in words[:-1]:\n",
    "        url += \"{}%20OR\".format(w)\n",
    "\n",
    "    url += \"{}%20\".format(words[-1])\n",
    "    url += \"since%3A{}%20until%3A{}&\".format(start_date, end_date)\n",
    "\n",
    "    if lang != 0:\n",
    "        url += \"l={}&\".format(languages[lang])\n",
    "\n",
    "    url += \"src=typd\"\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    start_time = time.time()  # remember when we started\n",
    "\n",
    "    while (time.time() - start_time) < max_time:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "def scrape_tweets(driver):\n",
    "    try:\n",
    "        tweet_divs = driver.page_source\n",
    "        obj = BeautifulSoup(tweet_divs, \"html.parser\")\n",
    "        content = obj.find_all(\"div\", class_=\"content\")\n",
    "        dates = []\n",
    "        names = []\n",
    "        tweet_texts = []\n",
    "        for i in content:\n",
    "            date = (i.find_all(\"span\", class_=\"_timestamp\")[0].string).strip()\n",
    "            try:\n",
    "                name = (i.find_all(\"strong\", class_=\"fullname\")[0].string).strip()\n",
    "            except AttributeError:\n",
    "                name = \"Anonymous\"\n",
    "\n",
    "            tweets = i.find(\"p\", class_=\"tweet-text\").strings\n",
    "            tweet_text = \"\".join(tweets)\n",
    "            # hashtags = i.find_all(\"a\", class_=\"twitter-hashtag\")[0].string\n",
    "            dates.append(date)\n",
    "            names.append(name)\n",
    "            tweet_texts.append(tweet_text)\n",
    "\n",
    "        data = {\n",
    "            \"date\": dates,\n",
    "            \"name\": names,\n",
    "            \"tweet\": tweet_texts,\n",
    "        }\n",
    "\n",
    "        make_csv(data)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Whoops! Something went wrong!\")\n",
    "        driver.quit()\n",
    "\n",
    "def make_csv(data):\n",
    "    l = len(data['date'])\n",
    "    print(\"count: %d\" % l)\n",
    "    with open(\"twitterData.csv\", \"a+\") as file:\n",
    "        fieldnames = ['Date', 'Name', 'Tweets']\n",
    "        writer = DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in range(l):\n",
    "            writer.writerow({'Date': data['date'][i],\n",
    "                             'Name': data['name'][i],\n",
    "                             'Tweets': data['tweet'][i],\n",
    "                            })\n",
    "\n",
    "def get_all_dates(start_date, end_date):\n",
    "    dates = []\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    step = timedelta(days=1)\n",
    "    \n",
    "    while start_date <= end_date:\n",
    "        dates.append(str(start_date.date()))\n",
    "        start_date += step\n",
    "    return dates\n",
    "\n",
    "def main():\n",
    "    driver_type = int(input(\"1) Firefox | 2) Chrome | 3) IE | 4) Opera | 5) PhantomJS\\nEnter the driver you want to use: \"))\n",
    "    wordsToSearch = input(\"Enter the words: \").split(',')\n",
    "    \n",
    "    for w in wordsToSearch:\n",
    "        w = w.strip()\n",
    "        \n",
    "    start_date = input(\"Enter the start date in (Y-M-D): \")\n",
    "    end_date = input(\"Enter the end date in (Y-M-D): \")\n",
    "    lang = int(input(\"0) All Languages 1) English | 2) Italian | 3) Spanish | 4) French | 5) German | 6) Russian | 7) Chinese | 8) Portuguese\\nEnter the language you want to use: \"))\n",
    "    all_dates = get_all_dates(start_date, end_date)\n",
    "    print(all_dates)\n",
    "\n",
    "    for i in range(len(all_dates) - 1):\n",
    "        driver = init_driver(driver_type)\n",
    "        scroll(driver, str(all_dates[i]), str(all_dates[i + 1]), wordsToSearch, lang)\n",
    "        scrape_tweets(driver)\n",
    "        time.sleep(5)\n",
    "        print(\"The tweets for {} are ready!\".format(all_dates[i]))\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
